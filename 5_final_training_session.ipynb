{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_training_session.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "notzH-f8_iRF",
        "colab_type": "text"
      },
      "source": [
        "# Image Super Resolution - Final Model Training\n",
        "Now that we have a set of hyperparameters chosen through the evolution algorithm, we are ready to train the final model with all available data and chosen hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IT-5Bx7aEma",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "a3fc5066-2b6f-4f7b-859b-701c3b11df24"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "import keras\n",
        "from keras.layers import Conv2D, UpSampling2D, Input, Add\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l1_l2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from numpy.random import randint\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVO1_ZnS7jy-",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65kLRiWK7WJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These are the hyperparameters chosen after the evolution algorithm\n",
        "h = {'l1_parameter': 0.01, \n",
        "     'l2_parameter': 0.016, \n",
        "     'num_residual_blocks': 9, \n",
        "     'num_conv_blocks': 2, \n",
        "     'num_final_conv_blocks': 3, \n",
        "     'num_epochs': 100, \n",
        "     'batch_size': 16, \n",
        "     'num_filters': 64, \n",
        "     'learning_rate': 0.0009, \n",
        "     'beta_1': 0.9, \n",
        "     'beta_2': 0.999}\n",
        "h.update({\n",
        "\t\"optimizer\" : keras.optimizers.Adam(lr=h['learning_rate'], beta_1=h['beta_1'], beta_2=h['beta_2'], amsgrad=False),\n",
        "\t'regularizer' : l1_l2(l1=h['l1_parameter'], l2=h['l2_parameter'])\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X6JuJOv7n3Y",
        "colab_type": "text"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX_qFkme7bN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a residual block\n",
        "def residual_block(input_layer, activation='relu', kernel_size=(3,3)):\n",
        "\tglobal h\n",
        "\tlayer = input_layer\n",
        "\tfor i in range(h['num_conv_blocks']):\n",
        "\t\tlayer = Conv2D(h['num_filters'], kernel_size, padding='same', activation=activation, activity_regularizer=h['regularizer'])(layer)\n",
        "\tconv_1x1 = Conv2D(3, (1,1), padding='same')(layer)\n",
        "\treturn Add()([conv_1x1, input_layer])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5feS-_-p7dHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# final convolution blocks\n",
        "def conv_block(input_layer, kernel_size=(3,3)):\n",
        "\tglobal h\n",
        "\tlayer = input_layer\n",
        "\tfor i in range(h['num_final_conv_blocks']):\n",
        "\t\tlayer = Conv2D(h['num_filters'], kernel_size, padding='same', activation='relu')(layer)\n",
        "\treturn layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPUgJzRV7fui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# upsamples 2x\n",
        "def upsample(layer):\n",
        "\treturn UpSampling2D(size=(2,2))(layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urXNAERJ7h3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# builds model based on hyperparameter specs\n",
        "def build_model(shape=(150,150,3)):\n",
        "  global h\n",
        "  input_layer = Input(shape)\n",
        "  layer = input_layer\n",
        "  for i in range(h['num_residual_blocks']):\n",
        "    layer = residual_block(layer)\n",
        "  layer = upsample(layer)\n",
        "  layer = conv_block(layer)\n",
        "  output_layer = Conv2D(3, (1,1), padding='same')(layer)\n",
        "\n",
        "  return Model(inputs=input_layer, outputs=output_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KI2b_9W7wKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function for getting image file names\n",
        "def get_filenames(directory):\n",
        "\tfor _,_,filenames in os.walk(directory):\n",
        "\t\tpass\n",
        "\treturn filenames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vik_dcRi7qzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# returns dataset in (x_train, y_train), (x_test, y_test) format\n",
        "from tqdm import tqdm\n",
        "\n",
        "def build_dataset(directory):\n",
        "\n",
        "\tfilenames = get_filenames(directory)\n",
        "\tX = []\n",
        "\tY = []\n",
        "\n",
        "\tfor filename in tqdm(filenames):\n",
        "\t\timage = Image.open(directory + filename)\n",
        "\t\timage_large = np.array(image)\n",
        "\t\timage_small = np.array(image.resize((150,150)))\n",
        "\t\tY.append(image_large)\n",
        "\t\tX.append(image_small)\n",
        "\n",
        "\tX = np.asarray(X)\n",
        "\tX = X.astype('float32')\n",
        "\tX /= 255\n",
        "\tY = np.asarray(Y)\n",
        "\tY = Y.astype('float32')\n",
        "\tY /= 255\n",
        "\n",
        "\treturn (X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaq_Sc46J4DI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" PSNR metric definition \"\"\"\n",
        "from keras import backend as K\n",
        "\n",
        "def PSNR(y_true, y_pred):\n",
        "    max_pixel = 1.0\n",
        "    return (10.0 * K.log((max_pixel ** 2) / (K.mean(K.square(y_pred - y_true), axis=-1)))) / 2.303"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6t46dHW76j_",
        "colab_type": "text"
      },
      "source": [
        "### Setup Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWGIEuMJaMBV",
        "colab_type": "code",
        "outputId": "041c1450-5de2-495d-fe1f-452a91e12449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\" Load Google Drive \"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48g6K2IaaNu4",
        "colab_type": "code",
        "outputId": "8e1686fe-8afe-4444-abd8-92013fb26b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\" Load dataset into memory \"\"\"\n",
        "directory = PATH_TO_DIRECTORY # this is defined locally\n",
        "X, Y = build_dataset(directory)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1087/1087 [08:59<00:00,  2.02it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaNGAeqnaWM5",
        "colab_type": "code",
        "outputId": "cb617974-5a70-4861-969c-33aecb9f5764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "import contextlib\n",
        "import json\n",
        "import warnings  \n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) # ignore ugly tensorflow deprecation warnings\n",
        "\n",
        "\n",
        "\"\"\" Choose a metric \"\"\"\n",
        "# metric = ['accuracy']\n",
        "metric = [PSNR]\n",
        "\n",
        "\"\"\" Build, compile and fit \"\"\"\n",
        "model = build_model(shape=(None,None,3))\n",
        "model.compile(loss='mae', optimizer=h['optimizer'], metrics=metric)\n",
        "model.fit(X, Y, batch_size=h['batch_size'], epochs=h['num_epochs'], verbose=1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1702: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "1087/1087 [==============================] - 67s 61ms/step - loss: 31588.0522 - PSNR: 16.2942\n",
            "Epoch 2/100\n",
            "1087/1087 [==============================] - 47s 44ms/step - loss: 10074.8899 - PSNR: 20.5816\n",
            "Epoch 3/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 5706.2219 - PSNR: 22.6168\n",
            "Epoch 4/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 1848.2731 - PSNR: 26.9881\n",
            "Epoch 5/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 576.1148 - PSNR: 31.4370\n",
            "Epoch 6/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 298.8187 - PSNR: 33.3192\n",
            "Epoch 7/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 162.5912 - PSNR: 34.7492\n",
            "Epoch 8/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 101.4014 - PSNR: 34.8086\n",
            "Epoch 9/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 69.1852 - PSNR: 35.6089\n",
            "Epoch 10/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 50.9846 - PSNR: 35.0636\n",
            "Epoch 11/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 39.4960 - PSNR: 35.8265\n",
            "Epoch 12/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 31.7279 - PSNR: 36.2875\n",
            "Epoch 13/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 26.0653 - PSNR: 36.7810\n",
            "Epoch 14/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 21.8018 - PSNR: 36.8357\n",
            "Epoch 15/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 18.5852 - PSNR: 35.8584\n",
            "Epoch 16/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 15.9830 - PSNR: 36.0788\n",
            "Epoch 17/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 13.9193 - PSNR: 35.5590\n",
            "Epoch 18/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 12.2231 - PSNR: 37.3884\n",
            "Epoch 19/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 10.8128 - PSNR: 36.7806\n",
            "Epoch 20/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 9.6299 - PSNR: 36.8994\n",
            "Epoch 21/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 8.6092 - PSNR: 36.7762\n",
            "Epoch 22/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 7.7497 - PSNR: 37.8884\n",
            "Epoch 23/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 7.0022 - PSNR: 37.8455\n",
            "Epoch 24/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 6.3551 - PSNR: 37.9170\n",
            "Epoch 25/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 5.7779 - PSNR: 37.6767\n",
            "Epoch 26/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 5.2824 - PSNR: 37.1334\n",
            "Epoch 27/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 4.8326 - PSNR: 36.0866\n",
            "Epoch 28/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 4.4478 - PSNR: 37.3060\n",
            "Epoch 29/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 4.0913 - PSNR: 37.0327\n",
            "Epoch 30/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 3.7776 - PSNR: 37.9758\n",
            "Epoch 31/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 3.4919 - PSNR: 38.0598\n",
            "Epoch 32/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 3.2395 - PSNR: 38.0394\n",
            "Epoch 33/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 3.0113 - PSNR: 37.7470\n",
            "Epoch 34/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 2.8030 - PSNR: 37.7206\n",
            "Epoch 35/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 2.6095 - PSNR: 38.2161\n",
            "Epoch 36/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 2.4370 - PSNR: 37.6914\n",
            "Epoch 37/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 2.2787 - PSNR: 38.2427\n",
            "Epoch 38/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 2.1301 - PSNR: 37.9982\n",
            "Epoch 39/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 1.9950 - PSNR: 38.4673\n",
            "Epoch 40/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 1.8724 - PSNR: 37.7704\n",
            "Epoch 41/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 1.7526 - PSNR: 37.5712\n",
            "Epoch 42/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 1.6556 - PSNR: 37.4473\n",
            "Epoch 43/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 1.5572 - PSNR: 37.8031\n",
            "Epoch 44/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 1.4675 - PSNR: 38.1012\n",
            "Epoch 45/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 1.3816 - PSNR: 38.7510\n",
            "Epoch 46/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 1.3036 - PSNR: 38.5725\n",
            "Epoch 47/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 1.2312 - PSNR: 37.9870\n",
            "Epoch 48/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 1.1645 - PSNR: 37.7642\n",
            "Epoch 49/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 1.1013 - PSNR: 37.8569\n",
            "Epoch 50/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 1.0421 - PSNR: 37.9572\n",
            "Epoch 51/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.9847 - PSNR: 38.1022\n",
            "Epoch 52/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.9324 - PSNR: 39.0424\n",
            "Epoch 53/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.8843 - PSNR: 38.9973\n",
            "Epoch 54/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.8392 - PSNR: 38.1984\n",
            "Epoch 55/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.7954 - PSNR: 38.8436\n",
            "Epoch 56/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.7550 - PSNR: 38.6293\n",
            "Epoch 57/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.7166 - PSNR: 37.9076\n",
            "Epoch 58/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.6816 - PSNR: 37.7397\n",
            "Epoch 59/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.6465 - PSNR: 38.9169\n",
            "Epoch 60/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.6157 - PSNR: 37.9118\n",
            "Epoch 61/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.5833 - PSNR: 38.9386\n",
            "Epoch 62/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.5566 - PSNR: 38.5945\n",
            "Epoch 63/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.5295 - PSNR: 38.2413\n",
            "Epoch 64/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.5035 - PSNR: 38.2688\n",
            "Epoch 65/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.4798 - PSNR: 38.1300\n",
            "Epoch 66/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.4571 - PSNR: 38.1913\n",
            "Epoch 67/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.4354 - PSNR: 38.4017\n",
            "Epoch 68/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.4172 - PSNR: 37.2054\n",
            "Epoch 69/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.3961 - PSNR: 38.7140\n",
            "Epoch 70/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.3786 - PSNR: 37.9240\n",
            "Epoch 71/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.3602 - PSNR: 38.6733\n",
            "Epoch 72/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.3435 - PSNR: 39.1427\n",
            "Epoch 73/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.3283 - PSNR: 38.7808\n",
            "Epoch 74/100\n",
            "1087/1087 [==============================] - 48s 45ms/step - loss: 0.3143 - PSNR: 38.3654\n",
            "Epoch 75/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.2990 - PSNR: 39.0200\n",
            "Epoch 76/100\n",
            "1087/1087 [==============================] - 48s 45ms/step - loss: 0.2869 - PSNR: 38.4163\n",
            "Epoch 77/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.2747 - PSNR: 38.1073\n",
            "Epoch 78/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.2621 - PSNR: 38.7842\n",
            "Epoch 79/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.2502 - PSNR: 38.9346\n",
            "Epoch 80/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.2399 - PSNR: 38.8135\n",
            "Epoch 81/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.2298 - PSNR: 38.6885\n",
            "Epoch 82/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.2204 - PSNR: 38.3996\n",
            "Epoch 83/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.2115 - PSNR: 37.6426\n",
            "Epoch 84/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.2018 - PSNR: 38.7008\n",
            "Epoch 85/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.1937 - PSNR: 38.5676\n",
            "Epoch 86/100\n",
            "1087/1087 [==============================] - 48s 45ms/step - loss: 0.1857 - PSNR: 38.4675\n",
            "Epoch 87/100\n",
            "1087/1087 [==============================] - 48s 45ms/step - loss: 0.1784 - PSNR: 38.2373\n",
            "Epoch 88/100\n",
            "1087/1087 [==============================] - 48s 45ms/step - loss: 0.1707 - PSNR: 38.8427\n",
            "Epoch 89/100\n",
            "1087/1087 [==============================] - 48s 45ms/step - loss: 0.1636 - PSNR: 38.9844\n",
            "Epoch 90/100\n",
            "1087/1087 [==============================] - 48s 45ms/step - loss: 0.1576 - PSNR: 38.4945\n",
            "Epoch 91/100\n",
            "1087/1087 [==============================] - 48s 45ms/step - loss: 0.1511 - PSNR: 38.9039\n",
            "Epoch 92/100\n",
            "1087/1087 [==============================] - 48s 45ms/step - loss: 0.1449 - PSNR: 39.1668\n",
            "Epoch 93/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.1404 - PSNR: 38.1818\n",
            "Epoch 94/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.1343 - PSNR: 38.6939\n",
            "Epoch 95/100\n",
            "1087/1087 [==============================] - 48s 45ms/step - loss: 0.1294 - PSNR: 38.5700\n",
            "Epoch 96/100\n",
            "1087/1087 [==============================] - 48s 45ms/step - loss: 0.1245 - PSNR: 38.3145\n",
            "Epoch 97/100\n",
            "1087/1087 [==============================] - 48s 45ms/step - loss: 0.1198 - PSNR: 38.6066\n",
            "Epoch 98/100\n",
            "1087/1087 [==============================] - 48s 45ms/step - loss: 0.1159 - PSNR: 38.0492\n",
            "Epoch 99/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.1111 - PSNR: 38.8131\n",
            "Epoch 100/100\n",
            "1087/1087 [==============================] - 49s 45ms/step - loss: 0.1075 - PSNR: 39.1282\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7cc8caccf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7SUffU9BSxi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32aac1aa-fe55-42da-fd18-54223d34a189"
      },
      "source": [
        "\"\"\" Save model \"\"\"\n",
        "model.save('model.h5')\n",
        "\n",
        "print(\"done\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}